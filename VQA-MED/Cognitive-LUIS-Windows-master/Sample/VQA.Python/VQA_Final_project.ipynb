{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA - MED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the efforts made for Visual Q&A based on the data set from VQA-Med 2018 contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs for VQA are:\n",
    "1. The question text \n",
    "2. The image\n",
    "\n",
    "The question text is being embedded into a feature vector using a pre-traing [globe file](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "In a similar manner the image is being processed using a pre trained deep NN (e.g. [VGG](http://qr.ae/TUTEKo) with initial wights of a pretrained [imagenet model](https://en.wikipedia.org/wiki/ImageNet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. [Preperations and helpers](#Preperations-and-helpers)\n",
    "1. [Collecting pre processing item](#Collecting-pre-processing-item)\n",
    "2. [Preprocessing and creating meta data](#Preprocessing-and-creating-meta-data)\n",
    "3. [Creating the model](#Creating-the-model)\n",
    "4. [Training the model](#Training-the-model)\n",
    "5. [Testing the model](#Testing-the-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperations and helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are just helpers & utils imports - feel free to skip..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parsers.utils import VerboseTimer\n",
    "from utils.os_utils import File, print_progress\n",
    "import time, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)\n",
    "\n",
    "def get_time_stamp():\n",
    "    now = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(now).strftime('%Y%m%d_%H%M_%S')\n",
    "    return ts\n",
    "from vqa_logger import logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting pre processing item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Download pre trained items & store their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Add down loading for glove file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:03][DEBUG] using embedding vector: en_core_web_sm\n",
      "[20:34:04][DEBUG] Got embedding\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "seq_length =    26\n",
    "embedding_dim = 384\n",
    "\n",
    "vectors = ['en_core_web_lg','en_core_web_md', 'en_core_web_sm']#'en_vectors_web_lg'\n",
    "vector = vectors[2]\n",
    "logger.debug(f'using embedding vector: {vector}')\n",
    "nlp = spacy.load('en', vectors=vector)\n",
    "# logger.debug(f'vector \"{vector}\" loaded')\n",
    "# logger.debug(f'nlp creating pipe')\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# logger.debug(f'nlpgetting embedding')\n",
    "# word_embeddings = nlp.vocab.vectors.data\n",
    "logger.debug(f'Got embedding')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# embedding_dim = 300\n",
    "# glove_path =                    os.path.abspath('data/glove.6B.{0}d.txt'.format(embedding_dim))\n",
    "# embedding_matrix_filename =     os.path.abspath('data/ckpts/embeddings_{0}.h5'.format(embedding_dim))\n",
    "ckpt_model_weights_filename =   os.path.abspath('data/ckpts/model_weights.h5')\n",
    "\n",
    "spacy_emmbeding_dim = 384\n",
    "\n",
    "\n",
    "DEFAULT_IMAGE_WIEGHTS = 'imagenet'\n",
    "#  Since VGG was trained as a image of 224x224, every new image\n",
    "# is required to go through the same transformation\n",
    "image_size_by_base_models = {'imagenet': (224, 224)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set locations for pre-training items to-be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre process results files\n",
    "data_prepo_meta            = os.path.abspath('data/my_data_prepro.json')\n",
    "data_prepo_meta_validation = os.path.abspath('data/my_data_prepro_validation.json')\n",
    "# Location of embediing pre trained matrix\n",
    "embedding_matrix_filename  = os.path.abspath('data/ckpts/embeddings_{0}.h5'.format(embedding_dim))\n",
    "\n",
    "# The location to dump models to\n",
    "vqa_models_folder          = \"C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\vqa_models\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and creating meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function for creating meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqa_logger import logger \n",
    "import itertools\n",
    "import string\n",
    "from utils.os_utils import File #This is a simplehelper file of mine...\n",
    "\n",
    "def create_meta(meta_file_location, df):\n",
    "        logger.debug(\"Creating meta data ('{0}')\".format(meta_file_location))\n",
    "        def get_unique_words(col):\n",
    "            single_string = \" \".join(df[col])\n",
    "            exclude = set(string.punctuation)\n",
    "            s_no_panctuation = ''.join(ch for ch in single_string if ch not in exclude)\n",
    "            unique_words = set(s_no_panctuation.split(\" \")).difference({'',' '})\n",
    "            print(\"column {0} had {1} unique words\".format(col,len(unique_words)))\n",
    "            return unique_words\n",
    "\n",
    "        cols = ['question', 'answer']\n",
    "        unique_words = set(itertools.chain.from_iterable([get_unique_words(col) for col in cols]))\n",
    "        print(\"total unique words: {0}\".format(len(unique_words)))\n",
    "\n",
    "        metadata = {}\n",
    "        metadata['ix_to_word'] = {str(word): int(i) for i, word in enumerate(unique_words)}\n",
    "        metadata['ix_to_ans'] = {ans:i for ans, i in enumerate(set(df['answer']))}\n",
    "        # {int(i):str(word) for i, word in enumerate(unique_words)}\n",
    "\n",
    "        File.dump_json(metadata,meta_file_location)\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets create meta data for training & validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dbg_file_csv_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA.csv'\n",
    "dbg_file_xls_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process_intermediate.xlsx'#\"'C:\\\\\\\\Users\\\\\\\\avitu\\\\\\\\Documents\\\\\\\\GitHub\\\\\\\\VQA-MED\\\\\\\\VQA-MED\\\\\\\\Cognitive-LUIS-Windows-master\\\\\\\\Sample\\\\\\\\VQA.Python\\\\\\\\dumped_data\\\\\\\\vqa_data.xlsx'\n",
    "dbg_file_xls_processed_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process.xlsx'\n",
    "train_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images\\\\embbeded_images.hdf'\n",
    "images_path_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA.csv'\n",
    "dbg_file_xls_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process.xlsx'\n",
    "validation_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images\\\\embbeded_images.hdf'\n",
    "images_path_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA.csv'\n",
    "dbg_file_xls_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process.xlsx'\n",
    "test_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images\\\\embbeded_images.hdf'\n",
    "images_path_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images'\n",
    "\n",
    "DataLocations = namedtuple('DataLocations', ['data_tag', 'raw_csv', 'raw_xls', 'processed_xls','images_path'])\n",
    "train_data = DataLocations('train', dbg_file_csv_train,dbg_file_xls_train,dbg_file_xls_processed_train, images_path_train)\n",
    "validation_data = DataLocations('validation', dbg_file_csv_validation, dbg_file_xls_validation, dbg_file_xls_processed_validation, images_path_validation)\n",
    "test_data = DataLocations('test', dbg_file_csv_test, dbg_file_xls_test, dbg_file_xls_processed_test, images_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data itself, Note the only things required in dataframe are:\n",
    "1. image_name\n",
    "2. question\n",
    "3. answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parsers.VQA18 import Vqa18Base\n",
    "df_train = Vqa18Base.get_instance(train_data.processed_xls).data            \n",
    "df_val = Vqa18Base.get_instance(validation_data.processed_xls).data\n",
    "# df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Creating training meta -----\n",
      "[20:34:05][DEBUG] Creating meta data ('C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\my_data_prepro.json')\n",
      "column question had 3317 unique words\n",
      "column answer had 3255 unique words\n",
      "total unique words: 3578\n",
      "\n",
      "----- Creating validation meta -----\n",
      "[20:34:05][DEBUG] Creating meta data ('C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\my_data_prepro.json')\n",
      "column question had 399 unique words\n",
      "column answer had 669 unique words\n",
      "total unique words: 881\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Creating training meta -----\")\n",
    "meta_train = create_meta(data_prepo_meta, df_train)\n",
    "\n",
    "print(\"\\n----- Creating validation meta -----\")\n",
    "meta_validation = create_meta(data_prepo_meta, df_val)\n",
    "\n",
    "# meta_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The functions the gets the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VqaSpecs(embedding_dim=384, seq_length=26, meta_data='"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "VqaSpecs = namedtuple('VqaSpecs',['embedding_dim', 'seq_length', 'meta_data'])\n",
    "def get_vqa_specs(meta_data):    \n",
    "    dim = embedding_dim\n",
    "    s_length = seq_length    \n",
    "    return VqaSpecs(embedding_dim=dim, seq_length=s_length, meta_data=meta_data)\n",
    "\n",
    "vqa_specs = get_vqa_specs(meta_train)\n",
    "s = str(vqa_specs)\n",
    "s[:s.index('meta_data=')+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how to build the word-to vector branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def word_2_vec_model(input_tensor):\n",
    "        # notes:\n",
    "        # num works: scalar represents size of original corpus\n",
    "        # embedding_dim : dim reduction. every input string will be encoded in a binary fashion using a vector of this length\n",
    "        # embedding_matrix (AKA embedding_initializers): represents a pre trained network\n",
    "\n",
    "        LSTM_UNITS = 512\n",
    "        DENSE_UNITS = 1024\n",
    "        DENSE_ACTIVATION = 'relu'\n",
    "\n",
    "\n",
    "        # logger.debug(\"Creating Embedding model\")\n",
    "        # x = Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=seq_length,trainable=False)(input_tensor)\n",
    "        # x = LSTM(units=LSTM_UNITS, return_sequences=True, input_shape=(seq_length, embedding_dim))(x)\n",
    "        # x = BatchNormalization()(x)\n",
    "        # x = LSTM(units=LSTM_UNITS, return_sequences=False)(x)\n",
    "        # x = BatchNormalization()(x)\n",
    "        x= input_tensor # Since using spacy\n",
    "        x = Dense(units=DENSE_UNITS, activation=DENSE_ACTIVATION)(x)\n",
    "        model = x\n",
    "        logger.debug(\"Done Creating Embedding model\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same manner, define how to build the image representation branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Dense, GlobalAveragePooling2D#, Input, Dropout\n",
    "def get_image_model(base_model_weights=DEFAULT_IMAGE_WIEGHTS, out_put_dim=1024):\n",
    "    base_model_weights = base_model_weights\n",
    "\n",
    "    # base_model = VGG19(weights=base_model_weights,include_top=False)\n",
    "    base_model = VGG19(weights=base_model_weights, include_top=False)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a global spatial average pooling layer\n",
    "    x = GlobalAveragePooling2D(name=\"image_model_average_pool\")(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(out_put_dim, activation='relu',name=\"image_model_dense\")(x)\n",
    "    # and a logistic layer -- let's say we have 200 classes\n",
    "    # predictions = Dense(200, activation='softmax')(x)\n",
    "    model = x\n",
    "    \n",
    "    return base_model.input , model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, just for making sure, lets clear the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as keras_backend\n",
    "keras_backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, building the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.layers as keras_layers\n",
    "#Available merge strategies:\n",
    "# keras_layers.multiply, keras_layers.add, keras_layers.concatenate, \n",
    "# keras_layers.average, keras_layers.co, keras_layers.dot, keras_layers.maximum\n",
    "            \n",
    "merge_strategy = keras_layers.concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:07][DEBUG] Getting embedding (lstm model)\n",
      "[20:34:07][DEBUG] Done Creating Embedding model\n",
      "[20:34:07][DEBUG] Getting image model\n",
      "[20:34:09][DEBUG] merging final model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x24a8769e5c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Model, models, Input, callbacks\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.layers import Dense, Embedding, LSTM, BatchNormalization#, GlobalAveragePooling2D, Merge, Flatten\n",
    "\n",
    "def get_vqa_model(meta):\n",
    "        DENSE_UNITS = 1000\n",
    "        DENSE_ACTIVATION = 'relu'\n",
    "\n",
    "        OPTIMIZER = 'rmsprop'\n",
    "        LOSS = 'categorical_crossentropy'\n",
    "        METRICS = 'accuracy'\n",
    "        num_classes = len(meta['ix_to_ans'].keys())\n",
    "        image_model, lstm_model, fc_model = None, None, None\n",
    "        try:\n",
    "\n",
    "            lstm_input_tensor = Input(shape=(embedding_dim,), name='embedding_input')\n",
    "\n",
    "            logger.debug(\"Getting embedding (lstm model)\")\n",
    "            lstm_model = word_2_vec_model(input_tensor=lstm_input_tensor)\n",
    "\n",
    "            logger.debug(\"Getting image model\")\n",
    "            out_put_dim = lstm_model.shape[-1].value\n",
    "            image_input_tensor, image_model = get_image_model(out_put_dim=out_put_dim)\n",
    "\n",
    "\n",
    "            logger.debug(\"merging final model\")\n",
    "            fc_tensors = merge_strategy(inputs=[image_model, lstm_model])\n",
    "            fc_tensors = BatchNormalization()(fc_tensors)\n",
    "            fc_tensors = Dense(units=DENSE_UNITS, activation=DENSE_ACTIVATION)(fc_tensors)\n",
    "            fc_tensors = BatchNormalization()(fc_tensors)\n",
    "            fc_tensors = Dense(units=num_classes, activation='softmax')(fc_tensors)\n",
    "\n",
    "            fc_model = Model(inputs=[lstm_input_tensor, image_input_tensor], output=fc_tensors)\n",
    "            fc_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS])\n",
    "        except Exception as ex:\n",
    "            logger.error(\"Got an error while building vqa model:\\n{0}\".format(ex))\n",
    "            models = [(image_model, 'image_model'), (lstm_model, 'lstm_model'), (fc_model, 'lstm_model')]\n",
    "            for m, name in models:\n",
    "                if m is not None:\n",
    "                    logger.error(\"######################### {0} model details: ######################### \".format(name))\n",
    "                    try:\n",
    "                        m.summary(print_fn=logger.error)\n",
    "                    except Exception as ex2:\n",
    "                        logger.warning(\"Failed to print summary for {0}:\\n{1}\".format(name, ex2))\n",
    "            raise\n",
    "\n",
    "        return fc_model\n",
    "\n",
    "model = get_vqa_model(meta_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)           (None, None, None, 2 590080      block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)           (None, None, None, 5 2359808     block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)           (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, None, None, 5 0           block5_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "image_model_average_pool (Globa (None, 512)          0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_input (InputLayer)    (None, 384)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_model_dense (Dense)       (None, 1024)         525312      image_model_average_pool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         394240      embedding_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2048)         0           image_model_dense[0][0]          \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2048)         8192        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         2049000     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1000)         4000        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4471)         4475471     batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 27,480,599\n",
      "Trainable params: 27,474,503\n",
      "Non-trainable params: 6,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We better save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:09][DEBUG] saving model to: 'C:\\Users\\Public\\Documents\\Data\\2018\\vqa_models\\20180611_2034_09\\vqa_model.h5'\n",
      "[20:34:10][DEBUG] model saved\n",
      "[20:34:10][DEBUG] Writing history\n",
      "[20:34:10][DEBUG] Done Writing History\n"
     ]
    }
   ],
   "source": [
    "def print_model_summary_to_file(fn, model):\n",
    "    # Open the file\n",
    "    with open(fn,'w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "        \n",
    "\n",
    "ts = get_time_stamp()\n",
    "\n",
    "now_folder = os.path.abspath('{0}\\\\{1}\\\\'.format(vqa_models_folder, ts))\n",
    "model_fn = os.path.join(now_folder, 'vqa_model.h5')\n",
    "model_image_fn = os.path.join(now_folder, 'model_vqa.png5')\n",
    "summary_fn = os.path.join(now_folder, 'model_summary.txt')\n",
    "logger.debug(\"saving model to: '{0}'\".format(model_fn))\n",
    "\n",
    "try:\n",
    "    File.validate_dir_exists(now_folder)\n",
    "    model.save(model_fn)  # creates a HDF5 file 'my_model.h5'\n",
    "    logger.debug(\"model saved\")\n",
    "except Exception as ex:\n",
    "    logger.error(\"Failed to save model:\\n{0}\".format(ex))\n",
    "\n",
    "try:\n",
    "    logger.debug(\"Writing history\")\n",
    "    print_model_summary_to_file(summary_fn, model)\n",
    "    logger.debug(\"Done Writing History\")\n",
    "#     logger.debug(\"Plotting model\")\n",
    "#     plot_model(model, to_file=model_image_fn)\n",
    "#     logger.debug(\"Done Plotting\")\n",
    "except Exception as ex:\n",
    "    logger.warning(\"{0}\".format(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "keras_backend.clear_session()\n",
    "\n",
    "def get_categorial_labels(df, meta):\n",
    "    classes = df['answer']\n",
    "    class_count = len(classes)\n",
    "    classes_indices = list(meta['ix_to_ans'].keys())\n",
    "    categorial_labels = to_categorical(classes_indices, num_classes=class_count)\n",
    "\n",
    "    return categorial_labels\n",
    "\n",
    "categorial_labels_train = get_categorial_labels(df_train, meta_train)\n",
    "categorial_labels_val = get_categorial_labels(df_val, meta_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:28][DEBUG] Building input dataframe\n",
      "[20:36:28][DEBUG] Getting image features\n",
      "[20:36:57][DEBUG] Getting questions embedding\n",
      "[20:38:02][DEBUG] Getting answers embedding\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model, to_categorical\n",
    "import cv2\n",
    "keras_backend.clear_session()\n",
    "\n",
    "def get_categorial_labels(df, meta):\n",
    "    classes = df['answer']\n",
    "    class_count = len(classes)\n",
    "    classes_indices = list(meta['ix_to_ans'].keys())\n",
    "    categorial_labels = to_categorical(classes_indices, num_classes=class_count)\n",
    "\n",
    "    return categorial_labels\n",
    "\n",
    "categorial_labels_train = get_categorial_labels(df_train, meta_train)\n",
    "categorial_labels_val = get_categorial_labels(df_val, meta_validation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_image(image_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the\n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    image_size = image_size_by_base_models[DEFAULT_IMAGE_WIEGHTS]\n",
    "    im = cv2.resize(cv2.imread(image_file_name), image_size)\n",
    "\n",
    "    # convert the image to RGBA\n",
    "    im = im.transpose((2, 0, 1))\n",
    "    return im\n",
    "\n",
    "\n",
    "def get_text_features(txt):\n",
    "    ''' For a given txt, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    tokens = nlp(txt)    \n",
    "    text_features = np.zeros((1, len(tokens), spacy_emmbeding_dim))\n",
    "\n",
    "    for j, token in enumerate(tokens):\n",
    "        # print(len(token.vector))\n",
    "        text_features[0,j,:] = token.vector\n",
    "\n",
    "    return text_features\n",
    "logger.debug('Building input dataframe')\n",
    "\n",
    "image_name_question = df_train[['image_name', 'question', 'answer']].copy()\n",
    "image_name_question['image_name'] = image_name_question['image_name']\\\n",
    "                                    .apply(lambda q: q if q.lower().endswith('.jpg') else q+'.jpg')\n",
    "\n",
    "image_name_question['path'] =  image_name_question['image_name']\\\n",
    "                                .apply(lambda name:os.path.join(train_data.images_path, name))\n",
    "\n",
    "existing_files = [os.path.join(train_data.images_path, fn) for fn in os.listdir(train_data.images_path)]\n",
    "image_name_question = image_name_question.loc[image_name_question['path'].isin(existing_files)]\n",
    "\n",
    "logger.debug('Getting image features')\n",
    "image_name_question['image'] = image_name_question['path']\\\n",
    "                                .apply(lambda im_path: get_image(im_path))\n",
    "\n",
    "logger.debug('Getting questions embedding')\n",
    "image_name_question['question_embedding'] = image_name_question['question']\\\n",
    "                                            .apply(lambda q: get_text_features(q))\n",
    "\n",
    "\n",
    "logger.debug('Getting answers embedding')\n",
    "image_name_question['answer_embedding'] = image_name_question['answer']\\\n",
    "                                            .apply(lambda q: get_text_features(q))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:02][DEBUG] Save the data\n"
     ]
    }
   ],
   "source": [
    "logger.debug(\"Save the data\")\n",
    "image_name_question = image_name_question.head(2)\n",
    "image_name_question.to_hdf('model_input.h5', key='df')    \n",
    "# store = HDFStore('model_input.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:03][DEBUG] Shape: (2, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>path</th>\n",
       "      <th>image</th>\n",
       "      <th>question_embedding</th>\n",
       "      <th>answer_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rjv03401.jpg</td>\n",
       "      <td>what does mri show?</td>\n",
       "      <td>lesion at tail of pancreas</td>\n",
       "      <td>C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...</td>\n",
       "      <td>[[[0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[[-1.8407480716705322, 2.5507988929748535, 0....</td>\n",
       "      <td>[[[2.7199699878692627, 0.11310356855392456, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIAN-14-313-g002.jpg</td>\n",
       "      <td>where does axial section mri abdomen show hypo...</td>\n",
       "      <td>in distal pancreas</td>\n",
       "      <td>C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...</td>\n",
       "      <td>[[[9, 9, 10, 9, 10, 10, 10, 10, 9, 10, 11, 11,...</td>\n",
       "      <td>[[[0.35850387811660767, 1.4076576232910156, -3...</td>\n",
       "      <td>[[[1.1828632354736328, 0.4119483232498169, -3....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name                                           question  \\\n",
       "0          rjv03401.jpg                                what does mri show?   \n",
       "1  AIAN-14-313-g002.jpg  where does axial section mri abdomen show hypo...   \n",
       "\n",
       "                       answer  \\\n",
       "0  lesion at tail of pancreas   \n",
       "1          in distal pancreas   \n",
       "\n",
       "                                                path  \\\n",
       "0  C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...   \n",
       "1  C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...   \n",
       "\n",
       "                                               image  \\\n",
       "0  [[[0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [[[9, 9, 10, 9, 10, 10, 10, 10, 9, 10, 11, 11,...   \n",
       "\n",
       "                                  question_embedding  \\\n",
       "0  [[[-1.8407480716705322, 2.5507988929748535, 0....   \n",
       "1  [[[0.35850387811660767, 1.4076576232910156, -3...   \n",
       "\n",
       "                                    answer_embedding  \n",
       "0  [[[2.7199699878692627, 0.11310356855392456, -0...  \n",
       "1  [[[1.1828632354736328, 0.4119483232498169, -3....  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if image_name_question is None:\n",
    "    logger.debug(\"Load the data\")\n",
    "    from pandas import HDFStore\n",
    "    store = HDFStore('model_input.h5')\n",
    "    image_name_question = store['df']  \n",
    "\n",
    "logger.debug(f\"Shape: {image_name_question.shape}\")\n",
    "image_name_question.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "categorial_labels_train\n",
    "categorial_labels_val\n",
    "model\n",
    "train_features = np.asanyarray(image_name_question['question_embedding'], image_name_question['image'])\n",
    "train_labels = image_name_question['answer_embedding']\n",
    "validation_data = (train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectedt shape: [(None, 384), (None, None, None, 3)]\n",
      "Actual shape:(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 5, 384)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_layers\n",
    "model.input_layers_node_indices\n",
    "model.input_layers_tensor_indices\n",
    "model.input_mask\n",
    "model.input_names\n",
    "\n",
    "\n",
    "model.inputs\n",
    "model.input\n",
    "model.input_spec\n",
    "print(f'Expectedt shape: {model.input_shape}')\n",
    "print(f'Actual shape:{train_features.shape}')\n",
    "# model.input_shape, np.concatenate(train_features).shape\n",
    "model.input_shape, train_features[0].shape,  train_features[1].shape\n",
    "image_name_question['question_embedding'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:39:34][ERROR] Got an error training model: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[array([[[-1.84074807e+00,  2.55079889e+00,  7.17853904e-01, ...,\n",
      "          3.44256192e-01,  4.24880803e-01,  4.09096360e-01],\n",
      "        [-1.33866024e+00,  1.96757329e+00, -3.89488196e+00, ...,\n",
      "...\n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "[20:39:34][ERROR] ==================================================================================================\n",
      "[20:39:34][ERROR] input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block3_conv4 (Conv2D)           (None, None, None, 2 590080      block3_conv3[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv4[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block4_conv4 (Conv2D)           (None, None, None, 5 2359808     block4_conv3[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv4[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block5_conv4 (Conv2D)           (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] block5_pool (MaxPooling2D)      (None, None, None, 5 0           block5_conv4[0][0]               \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] image_model_average_pool (Globa (None, 512)          0           block5_pool[0][0]                \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] embedding_input (InputLayer)    (None, 384)          0                                            \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] image_model_dense (Dense)       (None, 1024)         525312      image_model_average_pool[0][0]   \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] dense_1 (Dense)                 (None, 1024)         394240      embedding_input[0][0]            \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] concatenate_1 (Concatenate)     (None, 2048)         0           image_model_dense[0][0]          \n",
      "[20:39:34][ERROR]                                                                  dense_1[0][0]                    \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] batch_normalization_1 (BatchNor (None, 2048)         8192        concatenate_1[0][0]              \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] dense_2 (Dense)                 (None, 1000)         2049000     batch_normalization_1[0][0]      \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] batch_normalization_2 (BatchNor (None, 1000)         4000        dense_2[0][0]                    \n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n",
      "[20:39:34][ERROR] dense_3 (Dense)                 (None, 4471)         4475471     batch_normalization_2[0][0]      \n",
      "[20:39:34][ERROR] ==================================================================================================\n",
      "[20:39:34][ERROR] Total params: 27,480,599\n",
      "[20:39:34][ERROR] Trainable params: 27,474,503\n",
      "[20:39:34][ERROR] Non-trainable params: 6,096\n",
      "[20:39:34][ERROR] __________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[array([[[-1.84074807e+00,  2.55079889e+00,  7.17853904e-01, ...,\n          3.44256192e-01,  4.24880803e-01,  4.09096360e-01],\n        [-1.33866024e+00,  1.96757329e+00, -3.89488196e+00, ...,\n...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-823ce9ef7164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m                         \u001b[1;31m#epochs=epochs,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                         \u001b[1;31m#batch_size=batch_size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                         validation_data=validation_data)\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Got an error training model: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[array([[[-1.84074807e+00,  2.55079889e+00,  7.17853904e-01, ...,\n          3.44256192e-01,  4.24880803e-01,  4.09096360e-01],\n        [-1.33866024e+00,  1.96757329e+00, -3.89488196e+00, ...,\n..."
     ]
    }
   ],
   "source": [
    "# train_features = image_name_question\n",
    "# validation_data = (validation_features, categorial_validation_labels)\n",
    "\n",
    "## construct the image generator for data augmentation\n",
    "# aug = image.ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "#                                height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "#                                horizontal_flip=True, fill_mode=\"nearest\")\n",
    "# train_generator = aug.flow(train_features, categorial_train_labels)\n",
    "\n",
    "# stop_callback = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=1,mode='auto')\n",
    "\n",
    "try:\n",
    "#     history = model.fit_generator(train_generator,\n",
    "#                                   validation_data=validation_data,\n",
    "#                                   steps_per_epoch=len(train_features) // self.batch_size,\n",
    "#                                   epochs=self.epochs,\n",
    "#                                   verbose=1,\n",
    "#                                   callbacks=[stop_callback],\n",
    "#                                   class_weight=class_weight\n",
    "#                                   )\n",
    "    # verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "    history = model.fit(train_features,train_labels,\n",
    "                        #epochs=epochs,\n",
    "                        #batch_size=batch_size,\n",
    "                        validation_data=validation_data)\n",
    "except Exception as ex:\n",
    "    logger.error(\"Got an error training model: {0}\".format(ex))\n",
    "    model.summary(print_fn=logger.error)\n",
    "    raise\n",
    "# return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
