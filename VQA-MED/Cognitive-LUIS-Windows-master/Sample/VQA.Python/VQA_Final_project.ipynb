{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA - MED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demostraits the efforts made for Visual Q&A based on the data set from VQA-Med 2018 contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs for VQA are:\n",
    "1. The question text \n",
    "2. The image\n",
    "\n",
    "The question text is being embedded into a feature vector using a pre-traing [globe file](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "In a similar manner the image is being processed using a pre trained deep NN (e.g. [VGG](http://qr.ae/TUTEKo) with initial wights of a pretrained [imagenet model](https://en.wikipedia.org/wiki/ImageNet))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. [Preperations and helpers](#Preperations-and-helpers)\n",
    "1. [Collecting pre processing item](#Collecting-pre-processing-item)\n",
    "2. [Preprocessing and creating meta data](#Preprocessing-and-creating-meta-data)\n",
    "3. [Creating the model](#Creating-the-model)\n",
    "4. [Training the model](#Training-the-model)\n",
    "5. [Testing the model](#Testing-the-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperations and helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are just helpers & utils imports - feel free to skip..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parsers.utils import VerboseTimer\n",
    "from utils.os_utils import File, print_progress\n",
    "import time, datetime\n",
    "\n",
    "def get_time_stamp():\n",
    "    now = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(now).strftime('%Y%m%d_%H%M_%S')\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting pre processing item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Download pre trained items & store their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Add down loading for glove file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "seq_length =    26\n",
    "embedding_dim = 300\n",
    "\n",
    "glove_path =                    os.path.abspath('data/glove.6B.{0}d.txt'.format(embedding_dim))\n",
    "embedding_matrix_filename =     os.path.abspath('data/ckpts/embeddings_{0}.h5'.format(embedding_dim))\n",
    "ckpt_model_weights_filename =   os.path.abspath('data/ckpts/model_weights.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_IMAGE_WIEGHTS = 'imagenet'\n",
    "#  Since VGG was trained as a image of 224x224, every new image\n",
    "# is required to go through the same transformation\n",
    "image_size_by_base_models = {'imagenet': (224, 224)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated file locations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Fail fast...\n",
    "suffix = \"Failing fast:\\n\"\n",
    "assert os.path.isfile(glove_path), suffix+\"glove file does not exists:\\n{0}\".format(glove_path)\n",
    "# assert os.path.isfile(embedding_matrix_filename), suffix+\"Embedding matrix file does not exist:\\n{0}\".format(embedding_matrix_filename)\n",
    "assert os.path.isfile(ckpt_model_weights_filename), suffix+\"glove file does not exists:\\n{0}\".format(ckpt_model_weights_filename)\n",
    "\n",
    "print('Validated file locations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set locations for pre-training items to-be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre process results files\n",
    "data_prepo_meta            = os.path.abspath('data/my_data_prepro.json')\n",
    "data_prepo_meta_validation = os.path.abspath('data/my_data_prepro_validation.json')\n",
    "# Location of embediing pre trained matrix\n",
    "embedding_matrix_filename  = os.path.abspath('data/ckpts/embeddings_{0}.h5'.format(embedding_dim))\n",
    "\n",
    "# The location to dump models to\n",
    "vqa_models_folder          = \"C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\vqa_models\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and creating meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function for creating meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vqa_logger import logger \n",
    "import itertools\n",
    "import string\n",
    "from utils.os_utils import File #This is a simplehelper file of mine...\n",
    "\n",
    "def create_meta(meta_file_location, df):\n",
    "        logger.debug(\"Creating meta data ('{0}')\".format(meta_file_location))\n",
    "        def get_unique_words(col):\n",
    "            single_string = \" \".join(df[col])\n",
    "            exclude = set(string.punctuation)\n",
    "            s_no_panctuation = ''.join(ch for ch in single_string if ch not in exclude)\n",
    "            unique_words = set(s_no_panctuation.split(\" \")).difference({'',' '})\n",
    "            print(\"column {0} had {1} unique words\".format(col,len(unique_words)))\n",
    "            return unique_words\n",
    "\n",
    "        cols = ['question', 'answer']\n",
    "        unique_words = set(itertools.chain.from_iterable([get_unique_words(col) for col in cols]))\n",
    "        print(\"total unique words: {0}\".format(len(unique_words)))\n",
    "\n",
    "        metadata = {}\n",
    "        metadata['ix_to_word'] = {str(word): int(i) for i, word in enumerate(unique_words)}\n",
    "        metadata['ix_to_ans'] = {ans:i for ans, i in enumerate(set(df['answer']))}\n",
    "        # {int(i):str(word) for i, word in enumerate(unique_words)}\n",
    "\n",
    "        File.dump_json(metadata,meta_file_location)\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets create meta data for training & validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dbg_file_csv_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA.csv'\n",
    "dbg_file_xls_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process_intermediate.xlsx'#\"'C:\\\\\\\\Users\\\\\\\\avitu\\\\\\\\Documents\\\\\\\\GitHub\\\\\\\\VQA-MED\\\\\\\\VQA-MED\\\\\\\\Cognitive-LUIS-Windows-master\\\\\\\\Sample\\\\\\\\VQA.Python\\\\\\\\dumped_data\\\\\\\\vqa_data.xlsx'\n",
    "dbg_file_xls_processed_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process.xlsx'\n",
    "train_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images\\\\embbeded_images.hdf'\n",
    "images_path_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA.csv'\n",
    "dbg_file_xls_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process.xlsx'\n",
    "validation_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images\\\\embbeded_images.hdf'\n",
    "images_path_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA.csv'\n",
    "dbg_file_xls_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process.xlsx'\n",
    "test_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images\\\\embbeded_images.hdf'\n",
    "images_path_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images'\n",
    "\n",
    "DataLocations = namedtuple('DataLocations', ['data_tag', 'raw_csv', 'raw_xls', 'processed_xls','images_path'])\n",
    "train_data = DataLocations('train', dbg_file_csv_train,dbg_file_xls_train,dbg_file_xls_processed_train, images_path_train)\n",
    "validation_data = DataLocations('validation', dbg_file_csv_validation, dbg_file_xls_validation, dbg_file_xls_processed_validation, images_path_validation)\n",
    "test_data = DataLocations('test', dbg_file_csv_test, dbg_file_xls_test, dbg_file_xls_processed_test, images_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data itself, Note the only things required in dataframe are:\n",
    "1. image_name\n",
    "2. question\n",
    "3. answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parsers.VQA18 import Vqa18Base\n",
    "df_train = Vqa18Base.get_instance(train_data.processed_xls).data            \n",
    "df_val = Vqa18Base.get_instance(validation_data.processed_xls).data\n",
    "# df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:24:27,485 pythonVQA DEBUG ## Creating meta data ('C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\my_data_prepro.json')\n",
      "21:24:27,615 pythonVQA DEBUG ## Creating meta data ('C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\my_data_prepro.json')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Creating training meta -----\n",
      "column question had 3317 unique words\n",
      "column answer had 3255 unique words\n",
      "total unique words: 3578\n",
      "\n",
      "----- Creating validation meta -----\n",
      "column question had 399 unique words\n",
      "column answer had 669 unique words\n",
      "total unique words: 881\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Creating training meta -----\")\n",
    "meta_train = create_meta(data_prepo_meta, df_train)\n",
    "\n",
    "print(\"\\n----- Creating validation meta -----\")\n",
    "meta_validation = create_meta(data_prepo_meta, df_val)\n",
    "\n",
    "# meta_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The functions the gets the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "def prepare_embeddings(metadata):\n",
    "    embedding_filename = embedding_matrix_filename\n",
    "    num_words = len(metadata['ix_to_word'].keys())\n",
    "    dim_embedding = embedding_dim\n",
    "\n",
    "\n",
    "\n",
    "    logger.debug(\"Embedding Data...\")\n",
    "    # texts = df['question']\n",
    "\n",
    "    embeddings_index = {}\n",
    "    i = -1\n",
    "    line = \"NO DATA\"\n",
    "\n",
    "\n",
    "    glove_line_count = File.file_len(glove_path, encoding=\"utf8\")\n",
    "    def process_line(i, line):\n",
    "        print_progress(i, glove_line_count)\n",
    "        try:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            print_progress(i+1, glove_line_count)\n",
    "        except Exception as ex:\n",
    "            logger.error(\n",
    "                \"An error occurred while working on glove file [line {0}]:\\n\"\n",
    "                \"Line text:\\t{1}\\nGlove path:\\t{2}\\n\"\n",
    "                \"{3}\".format(\n",
    "                    i, line, glove_path, ex))\n",
    "            raise\n",
    "\n",
    "\n",
    "    # with open(glove_path, 'r') as glove_file:\n",
    "    with VerboseTimer(\"Embedding\"):\n",
    "        with open(glove_path, 'r', encoding=\"utf8\") as glove_file:\n",
    "            [process_line(i=i, line=line)for i, line in enumerate(glove_file)]\n",
    "\n",
    "\n",
    "\n",
    "    embedding_matrix = np.zeros((num_words, dim_embedding))\n",
    "    word_index = metadata['ix_to_word']\n",
    "\n",
    "    with VerboseTimer(\"Creating matrix\"):\n",
    "        embedding_tupl = ((word, i, embeddings_index.get(word)) for word, i in word_index.items())\n",
    "        embedded_with_values = [(word, i, embedding_vector) for word, i, embedding_vector in embedding_tupl if embedding_vector is not None]\n",
    "\n",
    "        for word, i, embedding_vector in embedded_with_values:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "    e = {tpl[0] for tpl in embedded_with_values}\n",
    "    w = set(word_index.keys())\n",
    "    words_with_no_embedding = w-e\n",
    "    rnd = random.sample(words_with_no_embedding , 5)\n",
    "    logger.debug(\"{0} words did not have embedding. e.g.:\\n{1}\".format(len(words_with_no_embedding),rnd))\n",
    "\n",
    "    with VerboseTimer(\"Dumping matrix\"):\n",
    "        with h5py.File(embedding_filename, 'w') as f:\n",
    "            f.create_dataset('embedding_matrix', data=embedding_matrix)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the embedding already exists, save yourself the time and just load it.  \n",
    "Otherwise - calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:48:27,313 pythonVQA DEBUG ## Embedding Data already exists. Loading...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(embedding_matrix_filename):\n",
    "    logger.debug(\"Embedding Data already exists. Loading...\")\n",
    "    with h5py.File(embedding_matrix_filename) as f:\n",
    "        embedding_train = np.array(f['embedding_matrix'])    \n",
    "else:\n",
    "    logger.debug(\"Calculating Embedding...\")\n",
    "    embedding_train = prepare_embeddings(meta_train)\n",
    "    \n",
    "embedding_matrix = embedding_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44398999,  0.12817   , -0.25246999, ..., -0.20043001,\n",
       "        -0.082191  , -0.06255   ],\n",
       "       [ 0.08561   ,  0.077471  , -1.01680005, ..., -0.30044001,\n",
       "         0.012508  ,  0.24875   ],\n",
       "       [-0.16277   ,  0.033858  , -0.39416999, ...,  0.20255999,\n",
       "        -0.17546999, -0.30397999],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.21359   ,  0.85279   ,  0.48688999, ..., -0.19047   ,\n",
       "        -0.058526  , -0.49094   ],\n",
       "       [ 0.72328001, -0.1178    , -0.022166  , ...,  0.49592999,\n",
       "        -0.16937999, -0.58451003]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets wrap it with related information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EmbeddingData(Embedding length:3578, Embedding dim: 300, seq length: 26, meta length: 2)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vqa_flow.data_structures import EmbeddingData\n",
    "def get_embedding_data(embedding_matrix, meta_data):    \n",
    "    dim = embedding_dim\n",
    "    s_length = seq_length    \n",
    "    return EmbeddingData(embedding_matrix=embedding_matrix,embedding_dim=dim, seq_length=s_length, meta_data=meta_data)\n",
    "\n",
    "embedding_train = get_embedding_data(embedding_matrix, meta_train)\n",
    "str(embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how to build the word-to vector branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_2_vec_model(embedding_matrix, num_words, embedding_dim, seq_length, input_tensor):\n",
    "        # notes:\n",
    "        # num works: scalar represents size of original corpus\n",
    "        # embedding_dim : dim reduction. every input string will be encoded in a binary fashion using a vector of this length\n",
    "        # embedding_matrix (AKA embedding_initializers): represents a pre trained network\n",
    "\n",
    "        LSTM_UNITS = 512\n",
    "        DENSE_UNITS = 1024\n",
    "        DENSE_ACTIVATION = 'relu'\n",
    "\n",
    "\n",
    "        logger.debug(\"Creating Embedding model\")\n",
    "        x = Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=seq_length,trainable=False)(input_tensor)\n",
    "        x = LSTM(units=LSTM_UNITS, return_sequences=True, input_shape=(seq_length, embedding_dim))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LSTM(units=LSTM_UNITS, return_sequences=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(units=DENSE_UNITS, activation=DENSE_ACTIVATION)(x)\n",
    "        model = x\n",
    "        logger.debug(\"Done Creating Embedding model\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same manner, define how to build the image representation branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.layers import Dense, GlobalAveragePooling2D#, Input, Dropout\n",
    "def get_image_model(base_model_weights=DEFAULT_IMAGE_WIEGHTS, out_put_dim=1024):\n",
    "    base_model_weights = base_model_weights\n",
    "\n",
    "    # base_model = VGG19(weights=base_model_weights,include_top=False)\n",
    "    base_model = VGG19(weights=base_model_weights, include_top=False)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    # add a global spatial average pooling layer\n",
    "    x = GlobalAveragePooling2D(name=\"image_model_average_pool\")(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = Dense(out_put_dim, activation='relu',name=\"image_model_dense\")(x)\n",
    "    # and a logistic layer -- let's say we have 200 classes\n",
    "    # predictions = Dense(200, activation='softmax')(x)\n",
    "    model = x\n",
    "    \n",
    "    return base_model.input , model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, just for making sure, lets clear the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as keras_backend\n",
    "keras_backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, building the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.layers as keras_layers\n",
    "#Available merge strategies:\n",
    "# keras_layers.multiply, keras_layers.add, keras_layers.concatenate, \n",
    "# keras_layers.average, keras_layers.co, keras_layers.dot, keras_layers.maximum\n",
    "            \n",
    "merge_strategy = keras_layers.concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:18:51,987 pythonVQA DEBUG ## Getting embedding (lstm model)\n",
      "21:18:51,989 pythonVQA DEBUG ## Creating Embedding model\n",
      "21:18:53,871 pythonVQA DEBUG ## Done Creating Embedding model\n",
      "21:18:53,872 pythonVQA DEBUG ## Getting image model\n",
      "21:18:54,855 pythonVQA DEBUG ## merging final model\n",
      "c:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x22a057b5208>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Model, models, Input, callbacks\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.layers import Dense, Embedding, LSTM, BatchNormalization#, GlobalAveragePooling2D, Merge, Flatten\n",
    "\n",
    "def get_vqa_model(embedding_data=None):        \n",
    "        embedding_matrix = embedding_data.embedding_matrix\n",
    "        num_words = embedding_data.num_words\n",
    "        num_classes = embedding_data.num_classes\n",
    "\n",
    "        DENSE_UNITS = 1000\n",
    "        DENSE_ACTIVATION = 'relu'\n",
    "\n",
    "        OPTIMIZER = 'rmsprop'\n",
    "        LOSS = 'categorical_crossentropy'\n",
    "        METRICS = 'accuracy'\n",
    "\n",
    "        image_model, lstm_model, fc_model = None, None, None\n",
    "        try:\n",
    "\n",
    "            lstm_input_tensor = Input(shape=(embedding_dim,), name='embedding_input')\n",
    "\n",
    "            logger.debug(\"Getting embedding (lstm model)\")\n",
    "            lstm_model = word_2_vec_model(embedding_matrix=embedding_matrix, num_words=num_words, embedding_dim=embedding_dim,\n",
    "                                               seq_length=seq_length, input_tensor=lstm_input_tensor)\n",
    "\n",
    "            logger.debug(\"Getting image model\")\n",
    "            out_put_dim = lstm_model.shape[-1].value\n",
    "            image_input_tensor, image_model = get_image_model(out_put_dim=out_put_dim)\n",
    "\n",
    "\n",
    "            logger.debug(\"merging final model\")\n",
    "            fc_tensors = merge_strategy(inputs=[image_model, lstm_model])\n",
    "            fc_tensors = BatchNormalization()(fc_tensors)\n",
    "            fc_tensors = Dense(units=DENSE_UNITS, activation=DENSE_ACTIVATION)(fc_tensors)\n",
    "            fc_tensors = BatchNormalization()(fc_tensors)\n",
    "            fc_tensors = Dense(units=num_classes, activation='softmax')(fc_tensors)\n",
    "\n",
    "            fc_model = Model(input=[lstm_input_tensor, image_input_tensor], output=fc_tensors)\n",
    "            fc_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS])\n",
    "        except Exception as ex:\n",
    "            logger.error(\"Got an error while building vqa model:\\n{0}\".format(ex))\n",
    "            models = [(image_model, 'image_model'), (lstm_model, 'lstm_model'), (fc_model, 'lstm_model')]\n",
    "            for m, name in models:\n",
    "                if m is not None:\n",
    "                    logger.error(\"######################### {0} model details: ######################### \".format(name))\n",
    "                    try:\n",
    "                        m.summary(print_fn=logger.error)\n",
    "                    except Exception as ex2:\n",
    "                        logger.warning(\"Failed to print summary for {0}:\\n{1}\".format(name, ex2))\n",
    "            raise\n",
    "\n",
    "        return fc_model\n",
    "\n",
    "model = get_vqa_model(embedding_data=embedding_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the summary of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, None, None, 6 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, None, None, 6 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, None, None, 6 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, None, None, 1 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, None, None, 1 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, None, None, 1 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, None, None, 2 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, None, None, 2 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, None, None, 2 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv4 (Conv2D)           (None, None, None, 2 590080      block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, None, None, 2 0           block3_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, None, None, 5 1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, None, None, 5 2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, None, None, 5 2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv4 (Conv2D)           (None, None, None, 5 2359808     block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, None, None, 5 0           block4_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, None, None, 5 2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_input (InputLayer)    (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, None, None, 5 2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 26, 300)      1073400     embedding_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, None, None, 5 2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 26, 512)      1665024     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv4 (Conv2D)           (None, None, None, 5 2359808     block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 26, 512)      2048        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, None, None, 5 0           block5_conv4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 512)          2099200     batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "image_model_average_pool (Globa (None, 512)          0           block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "image_model_dense (Dense)       (None, 1024)         525312      image_model_average_pool[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         525312      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2048)         0           image_model_dense[0][0]          \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2048)         8192        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         2049000     batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000)         4000        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4471)         4475471     batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 32,453,391\n",
      "Trainable params: 31,371,847\n",
      "Non-trainable params: 1,081,544\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We better save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:19:59,870 pythonVQA DEBUG ## saving model to: 'C:\\Users\\Public\\Documents\\Data\\2018\\vqa_models\\20180605_2119_59\\vqa_model.h5'\n",
      "21:20:00,739 pythonVQA DEBUG ## model saved\n",
      "21:20:00,740 pythonVQA DEBUG ## Writing history\n",
      "21:20:00,749 pythonVQA DEBUG ## Done Writing History\n",
      "21:20:00,749 pythonVQA DEBUG ## Plotting model\n",
      "21:20:00,760 pythonVQA WARNING ## Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "def print_model_summary_to_file(fn, model):\n",
    "    # Open the file\n",
    "    with open(fn,'w') as fh:\n",
    "        # Pass the file handle in as a lambda function to make it callable\n",
    "        model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "        \n",
    "\n",
    "ts = get_time_stamp()\n",
    "\n",
    "now_folder = os.path.abspath('{0}\\\\{1}\\\\'.format(vqa_models_folder, ts))\n",
    "model_fn = os.path.join(now_folder, 'vqa_model.h5')\n",
    "model_image_fn = os.path.join(now_folder, 'model_vqa.png5')\n",
    "summary_fn = os.path.join(now_folder, 'model_summary.txt')\n",
    "logger.debug(\"saving model to: '{0}'\".format(model_fn))\n",
    "\n",
    "try:\n",
    "    File.validate_dir_exists(now_folder)\n",
    "    model.save(model_fn)  # creates a HDF5 file 'my_model.h5'\n",
    "    logger.debug(\"model saved\")\n",
    "except Exception as ex:\n",
    "    logger.error(\"Failed to save model:\\n{0}\".format(ex))\n",
    "\n",
    "try:\n",
    "    logger.debug(\"Writing history\")\n",
    "    print_model_summary_to_file(summary_fn, model)\n",
    "    logger.debug(\"Done Writing History\")\n",
    "    logger.debug(\"Plotting model\")\n",
    "    plot_model(model, to_file=model_image_fn)\n",
    "    logger.debug(\"Done Plotting\")\n",
    "except Exception as ex:\n",
    "    logger.warning(\"{0}\".format(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
