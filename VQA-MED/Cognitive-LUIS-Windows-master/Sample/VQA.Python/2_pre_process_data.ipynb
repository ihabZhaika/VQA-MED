{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "from pandas import HDFStore\n",
    "from vqa_logger import logger\n",
    "from utils.os_utils import File\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_meta            = os.path.abspath('./data/meta_data.json')\n",
    "data_location      = os.path.abspath('./data/model_input.h5')\n",
    "vqa_specs_location = os.path.abspath('./data/vqa_specs.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = File.load_json(fn_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyStrategies(Enum):\n",
    "    NLP = 1\n",
    "    CATEGORIAL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Duplicate:\n",
    "spacy_emmbeding_dim = 384\n",
    "\n",
    "embedding_dim = 384\n",
    "\n",
    "# input_dim : the vocabulary size. This is how many unique words are represented in your corpus.\n",
    "# output_dim : the desired dimension of the word vector. For example, if output_dim = 100, then every word will be mapped onto a vector with 100 elements, whereas if output_dim = 300, then every word will be mapped onto a vector with 300 elements.\n",
    "# input_length : the length of your sequences. For example, if your data consists of sentences, then this variable represents how many words there are in a sentence. As disparate sentences typically contain different number of words, it is usually required to pad your sequences such that all sentences are of equal length. The keras.preprocessing.pad_sequence method can be used for this (https://keras.io/preprocessing/sequence/).\n",
    "input_length = 32 # longest question / answer was 28 words. Rounding up to a nice round number\n",
    "\n",
    "# ATTN - nlp vector: Arbitrary  selected for both question and asnwers\n",
    "embedded_sentence_length = input_length * embedding_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:12][DEBUG] using embedding vector: en_core_web_sm\n",
      "[20:33:13][DEBUG] Got embedding\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "seq_length =    26\n",
    "\n",
    "\n",
    "vectors = ['en_core_web_lg','en_core_web_md', 'en_core_web_sm']#'en_vectors_web_lg'\n",
    "vector = vectors[2]\n",
    "logger.debug(f'using embedding vector: {vector}')\n",
    "nlp = spacy.load('en', vectors=vector)\n",
    "# logger.debug(f'vector \"{vector}\" loaded')\n",
    "# logger.debug(f'nlp creating pipe')\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# logger.debug(f'nlpgetting embedding')\n",
    "# word_embeddings = nlp.vocab.vectors.data\n",
    "logger.debug(f'Got embedding')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# embedding_dim = 300\n",
    "# glove_path =                    os.path.abspath('data/glove.6B.{0}d.txt'.format(embedding_dim))\n",
    "# embedding_matrix_filename =     os.path.abspath('data/ckpts/embeddings_{0}.h5'.format(embedding_dim))\n",
    "ckpt_model_weights_filename =   os.path.abspath('data/ckpts/model_weights.h5')\n",
    "\n",
    "spacy_emmbeding_dim = 384\n",
    "\n",
    "embedding_dim = 384\n",
    "\n",
    "DEFAULT_IMAGE_WIEGHTS = 'imagenet'\n",
    "#  Since VGG was trained as a image of 224x224, every new image\n",
    "# is required to go through the same transformation\n",
    "image_size_by_base_models = {'imagenet': (224, 224)}\n",
    "\n",
    "# classify_strategy = ClassifyStrategies.CATEGORIAL\n",
    "classify_strategy = ClassifyStrategies.NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dbg_file_csv_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA.csv'\n",
    "dbg_file_xls_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process_intermediate.xlsx'#\"'C:\\\\\\\\Users\\\\\\\\avitu\\\\\\\\Documents\\\\\\\\GitHub\\\\\\\\VQA-MED\\\\\\\\VQA-MED\\\\\\\\Cognitive-LUIS-Windows-master\\\\\\\\Sample\\\\\\\\VQA.Python\\\\\\\\dumped_data\\\\\\\\vqa_data.xlsx'\n",
    "dbg_file_xls_processed_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process.xlsx'\n",
    "train_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images\\\\embbeded_images.hdf'\n",
    "images_path_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA.csv'\n",
    "dbg_file_xls_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process.xlsx'\n",
    "validation_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images\\\\embbeded_images.hdf'\n",
    "images_path_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA.csv'\n",
    "dbg_file_xls_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process.xlsx'\n",
    "test_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images\\\\embbeded_images.hdf'\n",
    "images_path_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images'\n",
    "\n",
    "DataLocations = namedtuple('DataLocations', ['data_tag', 'raw_csv', 'raw_xls', 'processed_xls','images_path'])\n",
    "train_data = DataLocations('train', dbg_file_csv_train,dbg_file_xls_train,dbg_file_xls_processed_train, images_path_train)\n",
    "validation_data = DataLocations('validation', dbg_file_csv_validation, dbg_file_xls_validation, dbg_file_xls_processed_validation, images_path_validation)\n",
    "test_data = DataLocations('test', dbg_file_csv_test, dbg_file_xls_test, dbg_file_xls_processed_test, images_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsers.VQA18 import Vqa18Base\n",
    "df_train = Vqa18Base.get_instance(train_data.processed_xls).data            \n",
    "df_val = Vqa18Base.get_instance(validation_data.processed_xls).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:13][DEBUG] Building input dataframe\n"
     ]
    }
   ],
   "source": [
    "logger.debug('Building input dataframe')\n",
    "cols = ['image_name', 'question', 'answer']\n",
    "\n",
    "image_name_question = df_train[cols].copy()\n",
    "image_name_question_val = df_val[cols].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is just for performance and quick debug cycles! remove before actual trainining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name_question = image_name_question.head(5)\n",
    "# image_name_question_val = image_name_question_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\conda_env\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "def get_text_features(txt):\n",
    "    ''' For a given txt, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    tokens = nlp(txt)    \n",
    "    text_features = np.zeros((1, input_length, spacy_emmbeding_dim))\n",
    "    \n",
    "    num_tokens_to_take = min([input_length, len(tokens)])\n",
    "    trimmed_tokens = tokens[:num_tokens_to_take]\n",
    "    \n",
    "    for j, token in enumerate(trimmed_tokens):\n",
    "        # print(len(token.vector))\n",
    "        text_features[0,j,:] = token.vector\n",
    "    # Bringing to shape of (1, input_length * spacy_emmbeding_dim)\n",
    "    ## ATTN - nlp vector:\n",
    "    text_features = np.reshape(text_features, (1, input_length * spacy_emmbeding_dim))\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_image(image_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the\n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    image_size = image_size_by_base_models[DEFAULT_IMAGE_WIEGHTS]\n",
    "    im = cv2.resize(cv2.imread(image_file_name), image_size)\n",
    "\n",
    "    # convert the image to RGBA\n",
    "#     im = im.transpose((2, 0, 1))\n",
    "    return im\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "def get_categorial_labels(df, meta):\n",
    "    ans_to_ix = meta['ans_to_ix']\n",
    "    all_classes =  ans_to_ix.keys()\n",
    "    data_classes = df['answer']\n",
    "    class_count = len(all_classes)\n",
    "\n",
    "    classes_indices = [ans_to_ix[ans] for ans in data_classes]\n",
    "    categorial_labels = to_categorical(classes_indices, num_classes=class_count)\n",
    "    \n",
    "    for i in range(len(categorial_labels)):\n",
    "        assert np.argmax(categorial_labels[i])== classes_indices[i], 'Expected to get argmax at index of label'\n",
    "    \n",
    "\n",
    "\n",
    "    return categorial_labels\n",
    "\n",
    "categorial_labels_train = get_categorial_labels(df_train, meta_data)\n",
    "categorial_labels_val = get_categorial_labels(df_val, meta_data)\n",
    "# categorial_labels_train.shape, categorial_labels_val.shape\n",
    "del df_train\n",
    "del df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process_raw_data(df, images_path):\n",
    "    df['image_name'] = df['image_name'].apply(lambda q: q if q.lower().endswith('.jpg') else q+'.jpg')\n",
    "\n",
    "    df['path'] =  df['image_name'].apply(lambda name:os.path.join(images_path, name))\n",
    "\n",
    "    existing_files = [os.path.join(images_path, fn) for fn in os.listdir(images_path)]\n",
    "    df = df.loc[df['path'].isin(existing_files)]\n",
    "\n",
    "\n",
    "    logger.debug('Getting questions embedding')\n",
    "    df['question_embedding'] = df['question'].apply(lambda q: get_text_features(q))\n",
    "\n",
    "\n",
    "    logger.debug('Getting answers embedding')\n",
    "    df['answer_embedding'] = df['answer'].apply(lambda q: get_text_features(q))\n",
    "\n",
    "    logger.debug('Getting image features')\n",
    "    df['image'] = df['path'].apply(lambda im_path: get_image(im_path))\n",
    "\n",
    "    logger.debug('Done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "This might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:16][DEBUG] ----===== Preproceccing train data =====----\n",
      "[20:33:16][DEBUG] Getting questions embedding\n",
      "[20:34:17][DEBUG] Getting answers embedding\n",
      "[20:35:28][DEBUG] Getting image features\n",
      "[20:36:02][DEBUG] Done\n"
     ]
    }
   ],
   "source": [
    "logger.debug('----===== Preproceccing train data =====----')\n",
    "image_locations = train_data.images_path\n",
    "image_name_question = pre_process_raw_data(image_name_question, image_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:36:02][DEBUG] ----===== Preproceccing validation data =====----\n",
      "[20:36:02][DEBUG] Getting questions embedding\n",
      "[20:36:09][DEBUG] Getting answers embedding\n",
      "[20:36:15][DEBUG] Getting image features\n",
      "[20:36:19][DEBUG] Done\n"
     ]
    }
   ],
   "source": [
    "logger.debug('----===== Preproceccing validation data =====----')\n",
    "image_locations = validation_data.images_path\n",
    "image_name_question_val = pre_process_raw_data(image_name_question_val, image_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>path</th>\n",
       "      <th>question_embedding</th>\n",
       "      <th>answer_embedding</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rjv03401.jpg</td>\n",
       "      <td>what does mri show?</td>\n",
       "      <td>lesion at tail of pancreas</td>\n",
       "      <td>C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...</td>\n",
       "      <td>[[-1.8407480716705322, 2.5507988929748535, 0.7...</td>\n",
       "      <td>[[2.7199699878692627, 0.11310356855392456, -0....</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIAN-14-313-g002.jpg</td>\n",
       "      <td>where does axial section mri abdomen show hypo...</td>\n",
       "      <td>in distal pancreas</td>\n",
       "      <td>C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...</td>\n",
       "      <td>[[0.35850387811660767, 1.4076576232910156, -3....</td>\n",
       "      <td>[[1.1828632354736328, 0.4119483232498169, -3.4...</td>\n",
       "      <td>[[[9, 9, 9], [9, 9, 9], [10, 10, 10], [9, 9, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name                                           question  \\\n",
       "0          rjv03401.jpg                                what does mri show?   \n",
       "1  AIAN-14-313-g002.jpg  where does axial section mri abdomen show hypo...   \n",
       "\n",
       "                       answer  \\\n",
       "0  lesion at tail of pancreas   \n",
       "1          in distal pancreas   \n",
       "\n",
       "                                                path  \\\n",
       "0  C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...   \n",
       "1  C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...   \n",
       "\n",
       "                                  question_embedding  \\\n",
       "0  [[-1.8407480716705322, 2.5507988929748535, 0.7...   \n",
       "1  [[0.35850387811660767, 1.4076576232910156, -3....   \n",
       "\n",
       "                                    answer_embedding  \\\n",
       "0  [[2.7199699878692627, 0.11310356855392456, -0....   \n",
       "1  [[1.1828632354736328, 0.4119483232498169, -3.4...   \n",
       "\n",
       "                                               image  \n",
       "0  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "1  [[[9, 9, 9], [9, 9, 9], [10, 10, 10], [9, 9, 9...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_question.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the data, so later on we don't need to compute it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"VqaSpecs(embedding_dim=384, seq_length=26, data_location='C:\\\\\\\\Users\\\\\\\\avitu\\\\\\\\Documents\\\\\\\\GitHub\\\\\\\\VQA-MED\\\\\\\\VQA-MED\\\\\\\\Cognitive-LUIS-Windows-master\\\\\\\\Sample\\\\\\\\VQA.Python\\\\\\\\data\\\\\\\\model_input.h5', meta_data=\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "VqaSpecs = namedtuple('VqaSpecs',['embedding_dim', 'seq_length', 'data_location','meta_data'])\n",
    "def get_vqa_specs(meta_data):    \n",
    "    dim = embedding_dim\n",
    "    s_length = seq_length    \n",
    "    return VqaSpecs(embedding_dim=dim, seq_length=s_length, data_location=data_location,meta_data=meta_data)\n",
    "\n",
    "vqa_specs = get_vqa_specs(meta_data)\n",
    "\n",
    "# Show waht we got...\n",
    "s = str(vqa_specs)\n",
    "s[:s.index('meta_data=')+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:18][DEBUG] Save the data\n",
      "[20:38:49][DEBUG] Saved to C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\model_input.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.debug(\"Save the data\")\n",
    "\n",
    "item_to_save = image_name_question\n",
    "# item_to_save = image_name_question.head(10)\n",
    "\n",
    "# remove if exists\n",
    "try:\n",
    "    os.remove(data_location)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with HDFStore(data_location) as store:\n",
    "    store['train']  = image_name_question\n",
    "    store['val']  = image_name_question_val\n",
    "    \n",
    "item_to_save.to_hdf(vqa_specs.data_location, key='df')    \n",
    "# store = HDFStore('model_input.h5')\n",
    "logger.debug(f\"Saved to {vqa_specs.data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:38:53][DEBUG] VQA Specs saved to:\n",
      "C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\vqa_specs.pkl\n"
     ]
    }
   ],
   "source": [
    "File.dump_pickle(vqa_specs, vqa_specs_location)\n",
    "logger.debug(f\"VQA Specs saved to:\\n{vqa_specs_location}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
