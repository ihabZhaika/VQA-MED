{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import os\n",
    "import numpy as np\n",
    "from pandas import HDFStore\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "\n",
    "from vqa_logger import logger\n",
    "from common.os_utils import File\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constatns import train_data, validation_data, data_location, fn_meta, vqa_specs_location\n",
    "from common.settings import nlp_vector, input_length, embedding_dim, image_size, seq_length\n",
    "from common.classes import VqaSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = File.load_json(fn_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f'using embedding vector: {nlp_vector }')\n",
    "nlp = spacy.load('en', vectors=nlp_vector)\n",
    "\n",
    "# logger.debug(f'vector \"{nlp_vector}\" loaded')\n",
    "# logger.debug(f'nlp creating pipe')\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# logger.debug(f'nlp getting embedding')\n",
    "# word_embeddings = nlp.vocab.vectors.data\n",
    "logger.debug(f'Got embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsers.VQA18 import Vqa18Base\n",
    "df_train = Vqa18Base.get_instance(train_data.processed_xls).data            \n",
    "df_val = Vqa18Base.get_instance(validation_data.processed_xls).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Building input dataframe')\n",
    "cols = ['image_name', 'question', 'answer']\n",
    "\n",
    "image_name_question = df_train[cols].copy()\n",
    "image_name_question_val = df_val[cols].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is just for performance and quick debug cycles! remove before actual trainining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_name_question = image_name_question.head(5)\n",
    "# image_name_question_val = image_name_question_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_text_features(txt):\n",
    "    ''' For a given txt, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    tokens = nlp(txt)    \n",
    "    text_features = np.zeros((1, input_length, embedding_dim))\n",
    "    \n",
    "    num_tokens_to_take = min([input_length, len(tokens)])\n",
    "    trimmed_tokens = tokens[:num_tokens_to_take]\n",
    "    \n",
    "    for j, token in enumerate(trimmed_tokens):\n",
    "        # print(len(token.vector))\n",
    "        text_features[0,j,:] = token.vector\n",
    "    # Bringing to shape of (1, input_length * embedding_dim)\n",
    "    ## ATTN - nlp vector:\n",
    "    text_features = np.reshape(text_features, (1, input_length * embedding_dim))\n",
    "    return text_features\n",
    "\n",
    "\n",
    "def get_image(image_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the\n",
    "    weights (filters) as a 1, 4096 dimension vector '''    \n",
    "    im = cv2.resize(cv2.imread(image_file_name), image_size)\n",
    "\n",
    "    # convert the image to RGBA\n",
    "#     im = im.transpose((2, 0, 1))\n",
    "    return im\n",
    "\n",
    "\n",
    "def get_categorial_labels(df, meta):\n",
    "    ans_to_ix = meta['ans_to_ix']\n",
    "    all_classes =  ans_to_ix.keys()\n",
    "    data_classes = df['answer']\n",
    "    class_count = len(all_classes)\n",
    "\n",
    "    classes_indices = [ans_to_ix[ans] for ans in data_classes]\n",
    "    categorial_labels = to_categorical(classes_indices, num_classes=class_count)\n",
    "    \n",
    "    for i in range(len(categorial_labels)):\n",
    "        assert np.argmax(categorial_labels[i])== classes_indices[i], 'Expected to get argmax at index of label'\n",
    "    \n",
    "\n",
    "\n",
    "    return categorial_labels\n",
    "\n",
    "categorial_labels_train = get_categorial_labels(df_train, meta_data)\n",
    "categorial_labels_val = get_categorial_labels(df_val, meta_data)\n",
    "# categorial_labels_train.shape, categorial_labels_val.shape\n",
    "del df_train\n",
    "del df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_process_raw_data(df, images_path):\n",
    "    df['image_name'] = df['image_name'].apply(lambda q: q if q.lower().endswith('.jpg') else q+'.jpg')\n",
    "\n",
    "    df['path'] =  df['image_name'].apply(lambda name:os.path.join(images_path, name))\n",
    "\n",
    "    existing_files = [os.path.join(images_path, fn) for fn in os.listdir(images_path)]\n",
    "    df = df.loc[df['path'].isin(existing_files)]\n",
    "\n",
    "\n",
    "    logger.debug('Getting questions embedding')\n",
    "    df['question_embedding'] = df['question'].apply(lambda q: get_text_features(q))\n",
    "\n",
    "\n",
    "    logger.debug('Getting answers embedding')\n",
    "    df['answer_embedding'] = df['answer'].apply(lambda q: get_text_features(q))\n",
    "\n",
    "    logger.debug('Getting image features')\n",
    "    df['image'] = df['path'].apply(lambda im_path: get_image(im_path))\n",
    "\n",
    "    logger.debug('Done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the actual pre processing\n",
    "Note:  \n",
    "This might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('----===== Preproceccing train data =====----')\n",
    "image_locations = train_data.images_path\n",
    "image_name_question = pre_process_raw_data(image_name_question, image_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('----===== Preproceccing validation data =====----')\n",
    "image_locations = validation_data.images_path\n",
    "image_name_question_val = pre_process_raw_data(image_name_question_val, image_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>path</th>\n",
       "      <th>question_embedding</th>\n",
       "      <th>answer_embedding</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rjv03401.jpg</td>\n",
       "      <td>what does mri show?</td>\n",
       "      <td>lesion at tail of pancreas</td>\n",
       "      <td>C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...</td>\n",
       "      <td>[[-1.8407480716705322, 2.5507988929748535, 0.7...</td>\n",
       "      <td>[[2.7199699878692627, 0.11310356855392456, -0....</td>\n",
       "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIAN-14-313-g002.jpg</td>\n",
       "      <td>where does axial section mri abdomen show hypo...</td>\n",
       "      <td>in distal pancreas</td>\n",
       "      <td>C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...</td>\n",
       "      <td>[[0.35850387811660767, 1.4076576232910156, -3....</td>\n",
       "      <td>[[1.1828632354736328, 0.4119483232498169, -3.4...</td>\n",
       "      <td>[[[9, 9, 9], [9, 9, 9], [10, 10, 10], [9, 9, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name                                           question  \\\n",
       "0          rjv03401.jpg                                what does mri show?   \n",
       "1  AIAN-14-313-g002.jpg  where does axial section mri abdomen show hypo...   \n",
       "\n",
       "                       answer  \\\n",
       "0  lesion at tail of pancreas   \n",
       "1          in distal pancreas   \n",
       "\n",
       "                                                path  \\\n",
       "0  C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...   \n",
       "1  C:\\Users\\Public\\Documents\\Data\\2018\\VQAMed2018...   \n",
       "\n",
       "                                  question_embedding  \\\n",
       "0  [[-1.8407480716705322, 2.5507988929748535, 0.7...   \n",
       "1  [[0.35850387811660767, 1.4076576232910156, -3....   \n",
       "\n",
       "                                    answer_embedding  \\\n",
       "0  [[2.7199699878692627, 0.11310356855392456, -0....   \n",
       "1  [[1.1828632354736328, 0.4119483232498169, -3.4...   \n",
       "\n",
       "                                               image  \n",
       "0  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
       "1  [[[9, 9, 9], [9, 9, 9], [10, 10, 10], [9, 9, 9...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_name_question.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the data, so later on we don't need to compute it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"VqaSpecs(embedding_dim=384, seq_length=26, data_location='C:\\\\\\\\Users\\\\\\\\avitu\\\\\\\\Documents\\\\\\\\GitHub\\\\\\\\VQA-MED\\\\\\\\VQA-MED\\\\\\\\Cognitive-LUIS-Windows-master\\\\\\\\Sample\\\\\\\\VQA.Python\\\\\\\\data\\\\\\\\model_input.h5', meta_data=\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vqa_specs(meta_data):    \n",
    "    dim = embedding_dim\n",
    "    s_length = seq_length    \n",
    "    return VqaSpecs(embedding_dim=dim, seq_length=s_length, data_location=data_location,meta_data=meta_data)\n",
    "\n",
    "vqa_specs = get_vqa_specs(meta_data)\n",
    "\n",
    "# Show waht we got...\n",
    "s = str(vqa_specs)\n",
    "s[:s.index('meta_data=')+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "logger.debug(\"Save the data\")\n",
    "\n",
    "item_to_save = image_name_question\n",
    "# item_to_save = image_name_question.head(10)\n",
    "\n",
    "# remove if exists\n",
    "try:\n",
    "    os.remove(data_location)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with HDFStore(data_location) as store:\n",
    "    store['train']  = image_name_question\n",
    "    store['val']  = image_name_question_val\n",
    "    \n",
    "item_to_save.to_hdf(vqa_specs.data_location, key='df')    \n",
    "# store = HDFStore('model_input.h5')\n",
    "logger.debug(f\"Saved to {vqa_specs.data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "File.dump_pickle(vqa_specs, vqa_specs_location)\n",
    "logger.debug(f\"VQA Specs saved to:\\n{vqa_specs_location}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
