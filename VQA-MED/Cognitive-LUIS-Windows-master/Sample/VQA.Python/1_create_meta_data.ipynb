{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre process results files\n",
    "fn_meta            = os.path.abspath('data/meta_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dbg_file_csv_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA.csv'\n",
    "dbg_file_xls_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process_intermediate.xlsx'#\"'C:\\\\\\\\Users\\\\\\\\avitu\\\\\\\\Documents\\\\\\\\GitHub\\\\\\\\VQA-MED\\\\\\\\VQA-MED\\\\\\\\Cognitive-LUIS-Windows-master\\\\\\\\Sample\\\\\\\\VQA.Python\\\\\\\\dumped_data\\\\\\\\vqa_data.xlsx'\n",
    "dbg_file_xls_processed_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-QA_post_pre_process.xlsx'\n",
    "train_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images\\\\embbeded_images.hdf'\n",
    "images_path_train = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Train\\\\VQAMed2018Train-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA.csv'\n",
    "dbg_file_xls_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-QA_post_pre_process.xlsx'\n",
    "validation_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images\\\\embbeded_images.hdf'\n",
    "images_path_validation = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Valid\\\\VQAMed2018Valid-images'\n",
    "\n",
    "\n",
    "dbg_file_csv_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA.csv'\n",
    "dbg_file_xls_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process_intermediate.xlsx'\n",
    "dbg_file_xls_processed_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-QA_post_pre_process.xlsx'\n",
    "test_embedding_path = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images\\\\embbeded_images.hdf'\n",
    "images_path_test = 'C:\\\\Users\\\\Public\\\\Documents\\\\Data\\\\2018\\\\VQAMed2018Test\\\\VQAMed2018Test-images'\n",
    "\n",
    "DataLocations = namedtuple('DataLocations', ['data_tag', 'raw_csv', 'raw_xls', 'processed_xls','images_path'])\n",
    "train_data = DataLocations('train', dbg_file_csv_train,dbg_file_xls_train,dbg_file_xls_processed_train, images_path_train)\n",
    "validation_data = DataLocations('validation', dbg_file_csv_validation, dbg_file_xls_validation, dbg_file_xls_processed_validation, images_path_validation)\n",
    "test_data = DataLocations('test', dbg_file_csv_test, dbg_file_xls_test, dbg_file_xls_processed_test, images_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and creating meta data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data itself, Note the only things required in dataframe are:\n",
    "1. image_name\n",
    "2. question\n",
    "3. answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsers.VQA18 import Vqa18Base\n",
    "df_train = Vqa18Base.get_instance(train_data.processed_xls).data            \n",
    "df_val = Vqa18Base.get_instance(validation_data.processed_xls).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function for creating meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqa_logger import logger \n",
    "import itertools\n",
    "import string\n",
    "from utils.os_utils import File #This is a simplehelper file of mine...\n",
    "\n",
    "def create_meta(meta_file_location, df):\n",
    "        logger.debug(\"Creating meta data ('{0}')\".format(meta_file_location))\n",
    "        print(f\"Dataframe had {len(df)} rows\")\n",
    "        def get_unique_words(col):\n",
    "            single_string = \" \".join(df[col])\n",
    "            exclude = set(string.punctuation)\n",
    "            s_no_panctuation = ''.join(ch for ch in single_string if ch not in exclude)\n",
    "            unique_words = set(s_no_panctuation.split(\" \")).difference({'',' '})\n",
    "            print(\"column {0} had {1} unique words\".format(col,len(unique_words)))\n",
    "            return unique_words\n",
    "\n",
    "        cols = ['question', 'answer']\n",
    "        df_unique_words = set(itertools.chain.from_iterable([get_unique_words(col) for col in cols]))\n",
    "        df_unique_answers = set(df['answer'])        \n",
    "\n",
    "        metadata = {}\n",
    "        metadata['ix_to_word'] = {str(word): int(i) for i, word in enumerate(df_unique_words)}\n",
    "        metadata['ix_to_ans'] = {i:ans for i, ans in enumerate(df_unique_answers)}\n",
    "        metadata['ans_to_ix'] = {ans:i for i, ans in enumerate(df_unique_answers)}\n",
    "                \n",
    "        \n",
    "        #------------------- Asserts\n",
    "        answers = metadata['ix_to_ans'].values()\n",
    "        words = metadata['ix_to_word'].values()\n",
    "        \n",
    "        assert len(set(answers)) == len(answers), 'Got duplicate answers'\n",
    "        assert len(set(words)) == len(words), 'Got duplicate words'        \n",
    "        \n",
    "        print(\"Meta number of unique answers: {0}\".format(len(set(metadata['ix_to_ans'].values()))))\n",
    "        print(\"Meta number of unique words: {0}\".format(len(set(metadata['ix_to_word'].values()))))\n",
    "\n",
    "        File.dump_json(metadata,meta_file_location)\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Creating meta -----\n",
      "[20:31:14][DEBUG] Creating meta data ('C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\meta_data.json')\n",
      "Dataframe had 5913 rows\n",
      "column question had 3374 unique words\n",
      "column answer had 3360 unique words\n",
      "Meta number of unique answers: 4906\n",
      "Meta number of unique words: 3727\n",
      "Meta file available at: C:\\Users\\avitu\\Documents\\GitHub\\VQA-MED\\VQA-MED\\Cognitive-LUIS-Windows-master\\Sample\\VQA.Python\\data\\meta_data.json\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Creating meta -----\")\n",
    "full_df = pd.concat([df_train, df_val])\n",
    "meta_data = create_meta(fn_meta, full_df)\n",
    "print(f\"Meta file available at: {fn_meta}\")\n",
    "# meta_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
